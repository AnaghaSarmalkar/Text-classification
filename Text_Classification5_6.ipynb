{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification5_6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z12s6IqQtS3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "60112922-5adc-4670-bd67-e9c0de9080a9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-vhvxWmQ3eY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import operator\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeY1O8wxQ5Ys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.getcwd()\n",
        "os.chdir('/content/drive/My Drive/NLP Assign 2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUB3iXBaQ62L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train = load_svmlight_file(\"aclImdb/train/labeledBow.feat\")\n",
        "X_test, y_test = load_svmlight_file(\"aclImdb/test/labeledBow.feat\")\n",
        "X_train=X_train.astype(dtype=np.uint16)\n",
        "X_test=X_test.astype(dtype=np.uint16)\n",
        "\n",
        "y_train = (y_train >= 7).astype(int)\n",
        "y_test = (y_test >= 7).astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRQOOSF6Q8ox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conf_matrix(pred, test):\n",
        "    conf_matrix={'TP':0,'TN':0,'FP':0,'FN':0}\n",
        "    \n",
        "    for i in range(len(test)):\n",
        "        if test[i]==1 and pred[i]==0:\n",
        "            conf_matrix['FN']+=1\n",
        "        else:\n",
        "            if test[i]==1 and pred[i]==1:\n",
        "                conf_matrix['TP']+=1\n",
        "            else:\n",
        "                if test[i]==0 and pred[i]==1:\n",
        "                    conf_matrix['FP']+=1\n",
        "                else:\n",
        "                    if test[i]==0 and pred[i]==0:\n",
        "                        conf_matrix['TN']+=1\n",
        "    return conf_matrix\n",
        "\n",
        "def get_accuracy(pred, test):\n",
        "    matrix=conf_matrix(pred,test)\n",
        "    accuracy=(matrix['TP']+matrix['TN'])/(matrix['TP']+matrix['TN']+matrix['FN']+matrix['FP'])\n",
        "    return accuracy\n",
        "\n",
        "def get_precision(pred, test):\n",
        "    matrix=conf_matrix(pred,test)\n",
        "    try:\n",
        "        precision=matrix['TP']/(matrix['TP']+matrix['FP'])\n",
        "    except:\n",
        "        precision=0\n",
        "            \n",
        "    return precision\n",
        "\n",
        "def get_recall(pred,test):\n",
        "    matrix=conf_matrix(pred,test)\n",
        "    try:\n",
        "        recall=matrix['TP']/(matrix['TP']+matrix['FN'])\n",
        "    except:\n",
        "        recall=0\n",
        "    return recall\n",
        "\n",
        "def get_f1_measure(pred,test):\n",
        "    precision=get_precision(pred,test)\n",
        "    recall=get_recall(pred,test)\n",
        "    try:\n",
        "        f1_measure= 2*precision*recall/(precision+recall)\n",
        "    except:\n",
        "        f1_measure=0\n",
        "    return f1_measure"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3tf94pFTNVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_arr=X_train.astype(dtype=np.uint16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyow6C3-SelU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a=np.unique(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEALiOz0Q_hO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "45653a55-0ecf-40dd-8b8f-0c368eac5c65"
      },
      "source": [
        "clf1 = MultinomialNB()\n",
        "clf1.partial_fit(X_train_arr, y_train, a)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLRc7NQzTczp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_train = clf1.predict(X_train_arr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm7j8nzNUf22",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation on Train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kfrkYJKUiAn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "6b3a6a27-8d84-4ae0-f92e-1f8e61fbef3c"
      },
      "source": [
        "accuracy=get_accuracy(pred_train,list(y_train))\n",
        "precision=get_precision(pred_train,list(y_train))\n",
        "recall=get_recall(pred_train,list(y_train))\n",
        "f1_measure=get_f1_measure(pred_train,list(y_train))\n",
        "print(\"Accuracy: \",accuracy)\n",
        "print(\"Precision: \",precision)\n",
        "print(\"Recall: \",recall)\n",
        "print(\"F-measure: \",f1_measure)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.89996\n",
            "Precision:  0.9315494173500216\n",
            "Recall:  0.86336\n",
            "F-measure:  0.8961594353331949\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVD_8WooUjqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test=X_test.astype(dtype=np.uint16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBupuGsjUnF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_arr=x_test.todense()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaSPwpjMUox0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = np.zeros((25000,4), dtype=np.uint16)\n",
        "Xtest=np.append(X_test_arr, z, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k8kwFvCUw1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_test = []\n",
        "chunkSize = 25\n",
        "chunks = np.split(Xtest, chunkSize, axis=0) \n",
        "for chunk in chunks:\n",
        "  op = clf1.predict(chunk)\n",
        "  pred_test.extend(op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QAmLa6VU0J5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "e4425f36-2c80-4019-833a-68e192bdf55d"
      },
      "source": [
        "accuracy=get_accuracy(pred_test,list(y_test))\n",
        "precision=get_precision(pred_test,list(y_test))\n",
        "recall=get_recall(pred_test,list(y_test))\n",
        "f1_measure=get_f1_measure(pred_test,list(y_test))\n",
        "print(\"Accuracy: \",accuracy)\n",
        "print(\"Precision: \",precision)\n",
        "print(\"Recall: \",recall)\n",
        "print(\"F-measure: \",f1_measure)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.8136\n",
            "Precision:  0.8590401172375893\n",
            "Recall:  0.75032\n",
            "F-measure:  0.8010077717994706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL3izwtRU3xg",
        "colab_type": "text"
      },
      "source": [
        "# Problem 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuCzvLhCU9Zy",
        "colab_type": "text"
      },
      "source": [
        "On comparison with all the models, Multinomial naive Bayes performed the best since the vocabulary size was very large and the features were distributed as word counts."
      ]
    }
  ]
}